{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# This is a sample Jupyter Notebook\n",
    "\n",
    "Below is an example of a code cell. \n",
    "Put your cursor into the cell and press Shift+Enter to execute it and select the next one, or click 'Run Cell' button.\n",
    "\n",
    "Press Double Shift to search everywhere for classes, files, tool windows, actions, and settings.\n",
    "\n",
    "To learn more about Jupyter Notebooks in PyCharm, see [help](https://www.jetbrains.com/help/pycharm/ipython-notebook-support.html).\n",
    "For an overview of PyCharm, go to Help -> Learn IDE features or refer to [our documentation](https://www.jetbrains.com/help/pycharm/getting-started.html)."
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T10:17:00.410998Z",
     "start_time": "2025-05-11T10:17:00.174248Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.src.metrics.accuracy_metrics import accuracy\n",
    "\n",
    "mp_hands = mp.solutions.hands.Hands(...)\n",
    "# Create Hands processor instance with static image mode\n",
    "hands = mp_hands.Hands(static_image_mode=True)"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T10:17:02.990079Z",
     "start_time": "2025-05-11T10:17:02.681080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_image(img_path):\n",
    "    \"\"\"\n",
    "    Process single image to extract hand landmarks\n",
    "    Args:\n",
    "        img_path: Path to input image\n",
    "    Returns:\n",
    "        List of normalized landmarks (x,y,z) or None if no hand detected\n",
    "    \"\"\"\n",
    "    # Read image using OpenCV\n",
    "    img = cv2.imread(img_path)\n",
    "    # Convert BGR to RGB (MediaPipe requires RGB)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    # Process image with MediaPipe\n",
    "    results = hands.process(img)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        return [coord for hand in results.multi_hand_landmarks\n",
    "                for landmark in hand.landmark\n",
    "                for coord in [landmark.x, landmark.y, landmark.z]]\n",
    "    return None"
   ],
   "id": "9f0a67630e5791d2",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T10:25:57.981111Z",
     "start_time": "2025-05-11T10:25:57.859718Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_landmarks_csv(dataset_path, output_csv):\n",
    "    \"\"\"\n",
    "    Generate CSV file containing image paths and corresponding landmarks\n",
    "    Args:\n",
    "        dataset_path: Root directory of gesture folders\n",
    "        output_csv: Path to output CSV file\n",
    "    \"\"\"\n",
    "    # Open CSV file in write mode\n",
    "    with open(output_csv, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        #header row\n",
    "        writer.writerow(['image_path', 'landmarks', 'gesture_label'])\n",
    "\n",
    "        # Loop through each gesture folder\n",
    "        for gesture_idx, gesture_name in enumerate(sorted(os.listdir(dataset_path))):\n",
    "            gesture_path = os.path.join(dataset_path, gesture_name)\n",
    "\n",
    "            # Process each image in gesture folder\n",
    "            for img_name in sorted(os.listdir(gesture_path)):\n",
    "                img_path = os.path.join(gesture_path, img_name)\n",
    "                landmarks = process_image(img_path)\n",
    "                if landmarks: #if detected\n",
    "                    # Write image path, landmarks, and numeric label\n",
    "                    writer.writerow([img_path, ','.join(map(str, landmarks)), gesture_idx])"
   ],
   "id": "f44899c3fb94f344",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T10:51:32.096884Z",
     "start_time": "2025-05-11T10:51:31.503020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GlovedHandDataset(tf.keras.utils.Sequence): #dataset is big for memory\n",
    "    def __init__(self, csv_path ,batch_size=32 ,shuffle=True):\n",
    "        \"\"\"\n",
    "        Initialize data generator\n",
    "        Args:\n",
    "            csv_path: Path to landmarks CSV\n",
    "            batch_size: Number of samples per batch\n",
    "            shuffle: Whether to shuffle data after each epoch\n",
    "        \"\"\"\n",
    "        # Read all data from CSV\n",
    "        with open(csv_path) as f:\n",
    "            self.data = list(csv.reader(f))[:1]  # Skip header row\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns number of batches per epoch\"\"\"\n",
    "        return int(np.ceil(len(self.data) / float(self.batch_size)))\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Shuffles data after each epoch if enabled\"\"\"\n",
    "        self.indices = np.arange(len(self.data))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get batch indices\n",
    "        batch_indices = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "\n",
    "        # Initialize batch arrays\n",
    "        batch_images = []\n",
    "        batch_landmarks = []\n",
    "        batch_labels = []\n",
    "\n",
    "        for i in batch_indices:\n",
    "            img_path, lm_str, label = self.data[i]\n",
    "\n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.resize(img, (256, 256)) #resize to expected input for rgb\n",
    "            img = (img / 127.5) - 1.0  #normalize to [-1,1] (mediaPipe standard)\n",
    "\n",
    "            #convert landmark string to numpy array\n",
    "            landmarks = np.array(lm_str.split(','), dtype=np.float32)\n",
    "\n",
    "            batch_images.append(img)\n",
    "            batch_landmarks.append(landmarks)\n",
    "            batch_labels.append(int(label))\n",
    "\n",
    "        #return batch as numpy arrays\n",
    "        return np.array(batch_images), {\n",
    "            'output_1': np.array(batch_landmarks),\n",
    "            'output_2': np.array(batch_labels),\n",
    "        }"
   ],
   "id": "d60575d63089458c",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T11:55:37.883707Z",
     "start_time": "2025-05-11T11:55:37.824200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_model(num_gestures=17):\n",
    "    \"\"\"\n",
    "    Build modified MediaPipe model with two outputs\n",
    "    Args:\n",
    "        num_gestures: Number of gesture classes (17 for CADDIAN)\n",
    "    Returns:\n",
    "        Compiled Keras model\n",
    "    \"\"\"\n",
    "    #load base MediaPipe model (pretrained)\n",
    "    base_model = mp.solutions.hands.Hands(\n",
    "        static_image_mode=True,\n",
    "        model_complexity=1,\n",
    "        min_detection_confidence=0.5\n",
    "    )\n",
    "\n",
    "    #Freeze all layers except last 3\n",
    "    for layer in base_model.layers[:-3]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    #create multi-output model\n",
    "    x = base_model.output\n",
    "    #Landmark prediction head (original task)\n",
    "    landmarks_out = tf.keras.layers.Dense(21*3, name = 'output_1')(x)\n",
    "\n",
    "    #Gesture classification head (new task)\n",
    "    gesture_out = tf.keras.layers.Dense(num_gestures, activation='softmax', name = 'output_2')(x)\n",
    "\n",
    "    model = tf.keras.Model(\n",
    "        inputs=base_model.input,\n",
    "        outputs=[landmarks_out, gesture_out]\n",
    "    )\n",
    "\n",
    "    #Compile with custom loss weights\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "        loss={\n",
    "            'output_1': 'mse',\n",
    "            'output_2': 'sparse_categorical_crossentropy',\n",
    "        },\n",
    "        loss_weights={'output_1': 0.3, 'output_2': 0.7},\n",
    "        metrics={'output_2' : 'accuracy'}\n",
    "    )\n",
    "\n",
    "    return model"
   ],
   "id": "64c191631f954d0",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Main execution\n",
    "if __name__ == '__main__':\n",
    "    #please change the paths as it is in your files\n",
    "    dataset_path = \"C:/Users/crese/Downloads/caddy-gestures-complete-v2-release-all-scenarios-fast.ai\"\n",
    "    output_csv_path = 'landmarks.csv'\n",
    "\n",
    "    generate_landmarks_csv(dataset_path, output_csv_path)\n",
    "\n",
    "    #step 2: create data generators\n",
    "    train_gen = GlovedHandDataset(output_csv_path)"
   ],
   "id": "2b74e78b4badaf76",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T11:51:56.870679Z",
     "start_time": "2025-05-11T11:51:56.149801Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import urllib.request\n",
    "\n",
    "# Download the model (if not already present)\n",
    "if not os.path.exists('hand_landmark_model.h5'):\n",
    "    url = 'https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task'\n",
    "    urllib.request.urlretrieve(url, 'hand_landmark_model.h5')"
   ],
   "id": "6ccec825e6ad246c",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T12:00:04.050317Z",
     "start_time": "2025-05-11T12:00:03.664190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "import tensorflowjs as tfjs\n",
    "\n",
    "# Convert MediaPipe model to TensorFlow\n",
    "tfjs.converters.convert_tf_saved_model(\n",
    "    'hand_landmarker.task',\n",
    "    'hand_landmarker_savedmodel'\n",
    ")"
   ],
   "id": "b3269fa5f09d444a",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflowjs'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[19]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtensorflow\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtf\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtensorflowjs\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtfjs\u001B[39;00m\n\u001B[32m      4\u001B[39m \u001B[38;5;66;03m# Convert MediaPipe model to TensorFlow\u001B[39;00m\n\u001B[32m      5\u001B[39m tfjs.converters.convert_tf_saved_model(\n\u001B[32m      6\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mhand_landmarker.task\u001B[39m\u001B[33m'\u001B[39m,\n\u001B[32m      7\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mhand_landmarker_savedmodel\u001B[39m\u001B[33m'\u001B[39m\n\u001B[32m      8\u001B[39m )\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'tensorflowjs'"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T11:55:44.844874Z",
     "start_time": "2025-05-11T11:55:44.673596Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "    #step 3: Build and train model\n",
    "    model = build_model()\n",
    "    model.fit(\n",
    "        train_gen,\n",
    "        epochs=10,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    #step 4: Save trained model\n",
    "    model.save('gloved_hand_model.h5')"
   ],
   "id": "6c194d9f34e6d1f2",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Hands' object has no attribute 'layers'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAttributeError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[17]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m#step 3: Build and train model\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m model = \u001B[43mbuild_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      3\u001B[39m model.fit(\n\u001B[32m      4\u001B[39m     train_gen,\n\u001B[32m      5\u001B[39m     epochs=\u001B[32m10\u001B[39m,\n\u001B[32m      6\u001B[39m     verbose=\u001B[32m1\u001B[39m\n\u001B[32m      7\u001B[39m )\n\u001B[32m      9\u001B[39m \u001B[38;5;66;03m#step 4: Save trained model\u001B[39;00m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[16]\u001B[39m\u001B[32m, line 16\u001B[39m, in \u001B[36mbuild_model\u001B[39m\u001B[34m(num_gestures)\u001B[39m\n\u001B[32m     10\u001B[39m base_model = mp.solutions.hands.Hands(\n\u001B[32m     11\u001B[39m     static_image_mode=\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[32m     12\u001B[39m     model_complexity=\u001B[32m1\u001B[39m\n\u001B[32m     13\u001B[39m )\n\u001B[32m     15\u001B[39m \u001B[38;5;66;03m#Freeze all layers except last 3\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m16\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m \u001B[43mbase_model\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlayers\u001B[49m[:-\u001B[32m3\u001B[39m]:\n\u001B[32m     17\u001B[39m     layer.trainable = \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m     19\u001B[39m \u001B[38;5;66;03m#create multi-output model\u001B[39;00m\n",
      "\u001B[31mAttributeError\u001B[39m: 'Hands' object has no attribute 'layers'"
     ]
    }
   ],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
